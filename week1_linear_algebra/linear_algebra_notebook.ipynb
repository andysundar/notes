{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra for Data Science\n",
    "\n",
    "## Learning Objectives\n",
    "* Understand vectors, matrices, dot product, matrix multiplication\n",
    "* Apply concepts to PCA and neural networks\n",
    "\n",
    "## Table of Contents\n",
    "1. [Scalars, Vectors, Matrices, Tensors](#section1)\n",
    "2. [Matrix Operations](#section2)\n",
    "3. [Eigenvalues and Eigenvectors](#section3)\n",
    "4. [Programming Exercise: Matrix Operations](#section4)\n",
    "5. [Programming Exercise: PCA Implementation](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Scalars, Vectors, Matrices, Tensors\n",
    "\n",
    "### Scalars\n",
    "- A scalar is a single number\n",
    "- Denoted in lowercase italics: $a, b, c$\n",
    "- Examples: Temperature (25°C), Weight (65 kg)\n",
    "\n",
    "### Vectors\n",
    "- A vector is an ordered array of numbers\n",
    "- Can be thought of as points in space or directions\n",
    "- Denoted in bold lowercase: $\\mathbf{a}, \\mathbf{b}, \\mathbf{c}$ or with an arrow: $\\vec{a}$\n",
    "- Examples: \n",
    "  - 2D vector: $\\mathbf{v} = [3, 4]$ (a point in 2D space)\n",
    "  - 3D vector: $\\mathbf{v} = [3, 4, 5]$ (a point in 3D space)\n",
    "\n",
    "#### Vector operations\n",
    "- **Addition**: Add corresponding elements\n",
    "  - $\\mathbf{a} + \\mathbf{b} = [a_1 + b_1, a_2 + b_2, ..., a_n + b_n]$\n",
    "- **Scalar multiplication**: Multiply each element by scalar\n",
    "  - $c\\mathbf{a} = [c \\cdot a_1, c \\cdot a_2, ..., c \\cdot a_n]$\n",
    "- **Dot product**: Sum of element-wise products\n",
    "  - $\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + ... + a_nb_n = \\sum_{i=1}^{n} a_i b_i$\n",
    "- **Norm (magnitude)**: Length of the vector\n",
    "  - $\\|\\mathbf{a}\\| = \\sqrt{a_1^2 + a_2^2 + ... + a_n^2} = \\sqrt{\\mathbf{a} \\cdot \\mathbf{a}}$\n",
    "\n",
    "### Matrices\n",
    "- A matrix is a 2D array of numbers\n",
    "- Denoted in bold uppercase: $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$\n",
    "- Size described as rows × columns\n",
    "- Example: \n",
    "  - $\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$ is a 2×3 matrix\n",
    "\n",
    "#### Matrix notation\n",
    "- $a_{ij}$ refers to the element in the $i$-th row and $j$-th column\n",
    "- $\\mathbf{A}_{i,:}$ refers to the $i$-th row of $\\mathbf{A}$\n",
    "- $\\mathbf{A}_{:,j}$ refers to the $j$-th column of $\\mathbf{A}$\n",
    "\n",
    "### Tensors\n",
    "- Tensors are generalizations of matrices to higher dimensions\n",
    "- A scalar is a 0-order tensor\n",
    "- A vector is a 1st-order tensor\n",
    "- A matrix is a 2nd-order tensor\n",
    "- Higher-order tensors have more dimensions\n",
    "- Common in deep learning for representing complex data like images (3D tensors) or videos (4D tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Matrix Operations\n",
    "\n",
    "### Matrix Addition and Subtraction\n",
    "- Element-wise operation between matrices of the same dimensions\n",
    "- $\\mathbf{C} = \\mathbf{A} + \\mathbf{B}$ means $c_{ij} = a_{ij} + b_{ij}$\n",
    "\n",
    "### Scalar Multiplication\n",
    "- Multiply each element by a scalar\n",
    "- $\\mathbf{C} = c\\mathbf{A}$ means $c_{ij} = c \\cdot a_{ij}$\n",
    "\n",
    "### Matrix Multiplication\n",
    "- Not element-wise! Uses dot products between rows and columns\n",
    "- For $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$:\n",
    "  - $\\mathbf{A}$ must have dimensions $m \\times n$\n",
    "  - $\\mathbf{B}$ must have dimensions $n \\times p$\n",
    "  - $\\mathbf{C}$ will have dimensions $m \\times p$\n",
    "  - Each element $c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}$\n",
    "\n",
    "### Matrix Transpose\n",
    "- Flips a matrix over its diagonal\n",
    "- If $\\mathbf{B} = \\mathbf{A}^T$, then $b_{ij} = a_{ji}$\n",
    "- Changes an $m \\times n$ matrix to an $n \\times m$ matrix\n",
    "\n",
    "### Identity Matrix\n",
    "- Denoted as $\\mathbf{I}$\n",
    "- Square matrix with 1's on the diagonal and 0's elsewhere\n",
    "- Property: $\\mathbf{A} \\mathbf{I} = \\mathbf{I} \\mathbf{A} = \\mathbf{A}$\n",
    "\n",
    "### Matrix Inverse\n",
    "- Only defined for square matrices\n",
    "- $\\mathbf{A}^{-1}$ is the inverse of $\\mathbf{A}$ if $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}$\n",
    "- Not all matrices have inverses (singular matrices)\n",
    "\n",
    "### Matrix Determinant\n",
    "- Only defined for square matrices\n",
    "- Denoted as $\\det(\\mathbf{A})$ or $|\\mathbf{A}|$\n",
    "- A matrix is invertible if and only if its determinant is non-zero\n",
    "\n",
    "### Trace\n",
    "- Sum of diagonal elements\n",
    "- $\\text{tr}(\\mathbf{A}) = \\sum_{i} a_{ii}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Eigenvalues and Eigenvectors\n",
    "\n",
    "### Definition\n",
    "- An eigenvector $\\mathbf{v}$ of a square matrix $\\mathbf{A}$ is a non-zero vector that, when multiplied by $\\mathbf{A}$, results in a scalar multiple of itself\n",
    "- The scalar is called the eigenvalue $\\lambda$\n",
    "- Mathematically: $\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$\n",
    "\n",
    "### Calculating Eigenvalues and Eigenvectors\n",
    "1. Rearrange to $\\mathbf{A}\\mathbf{v} - \\lambda \\mathbf{v} = \\mathbf{0}$\n",
    "2. Factor out $\\mathbf{v}$: $(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0}$\n",
    "3. For non-trivial solutions, $\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0$\n",
    "4. Solve this characteristic equation to find eigenvalues\n",
    "5. For each eigenvalue, solve $(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0}$ to find eigenvectors\n",
    "\n",
    "### Importance in Data Science\n",
    "- Eigendecomposition: $\\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1}$\n",
    "  - Where $\\mathbf{Q}$ contains eigenvectors as columns\n",
    "  - $\\mathbf{\\Lambda}$ is a diagonal matrix of eigenvalues\n",
    "- Applications:\n",
    "  - Principal Component Analysis (PCA)\n",
    "  - Dimensionality reduction\n",
    "  - PageRank algorithm (Google's original ranking algorithm)\n",
    "  - Covariance matrices in statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Programming Exercise: Matrix Operations\n",
    "\n",
    "Let's implement some basic matrix operations from scratch, without using NumPy's matrix functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import NumPy for creating arrays, but we'll implement operations ourselves\n",
    "import numpy as np\n",
    "\n",
    "# Define some matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement matrix addition\n",
    "def matrix_add(A, B):\n",
    "    # Check if dimensions match\n",
    "    if A.shape != B.shape:\n",
    "        raise ValueError(\"Matrices must have the same dimensions for addition\")\n",
    "    \n",
    "    # Initialize result matrix with zeros\n",
    "    result = np.zeros(A.shape)\n",
    "    \n",
    "    # Add corresponding elements\n",
    "    for i in range(A.shape[0]):  # rows\n",
    "        for j in range(A.shape[1]):  # columns\n",
    "            result[i, j] = A[i, j] + B[i, j]\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Test matrix addition\n",
    "C = matrix_add(A, B)\n",
    "print(\"A + B =\")\n",
    "print(C)\n",
    "\n",
    "# Verify with NumPy\n",
    "print(\"\\nVerify with NumPy:\")\n",
    "print(A + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement matrix multiplication\n",
    "def matrix_multiply(A, B):\n",
    "    # Check if dimensions are compatible\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise ValueError(\"Number of columns in A must equal number of rows in B\")\n",
    "    \n",
    "    # Initialize result matrix with zeros\n",
    "    result = np.zeros((A.shape[0], B.shape[1]))\n",
    "    \n",
    "    # Multiply matrices\n",
    "    for i in range(A.shape[0]):  # rows of A\n",
    "        for j in range(B.shape[1]):  # columns of B\n",
    "            for k in range(A.shape[1]):  # columns of A / rows of B\n",
    "                result[i, j] += A[i, k] * B[k, j]\n",
    "                \n",
    "    return result\n",
    "\n",
    "# Test matrix multiplication\n",
    "D = matrix_multiply(A, B)\n",
    "print(\"A × B =\")\n",
    "print(D)\n",
    "\n",
    "# Verify with NumPy\n",
    "print(\"\\nVerify with NumPy:\")\n",
    "print(np.matmul(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement matrix transpose\n",
    "def matrix_transpose(A):\n",
    "    # Initialize result matrix with zeros\n",
    "    result = np.zeros((A.shape[1], A.shape[0]))\n",
    "    \n",
    "    # Transpose matrix\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            result[j, i] = A[i, j]\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Test matrix transpose\n",
    "A_T = matrix_transpose(A)\n",
    "print(\"A^T =\")\n",
    "print(A_T)\n",
    "\n",
    "# Verify with NumPy\n",
    "print(\"\\nVerify with NumPy:\")\n",
    "print(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dot product\n",
    "def dot_product(v1, v2):\n",
    "    # Check if dimensions match\n",
    "    if len(v1) != len(v2):\n",
    "        raise ValueError(\"Vectors must have the same dimension\")\n",
    "    \n",
    "    # Calculate dot product\n",
    "    result = 0\n",
    "    for i in range(len(v1)):\n",
    "        result += v1[i] * v2[i]\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Test dot product\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "dot = dot_product(v1, v2)\n",
    "print(f\"Dot product of {v1} and {v2} = {dot}\")\n",
    "\n",
    "# Verify with NumPy\n",
    "print(f\"Verify with NumPy: {np.dot(v1, v2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Programming Exercise: PCA Implementation\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique widely used in machine learning. We'll implement it from scratch to understand the underlying linear algebra concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset for testing\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"First few samples:\\n{X[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_from_scratch(X, num_components):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis (PCA) from scratch\n",
    "    \n",
    "    Parameters:\n",
    "    X: Input data matrix (samples × features)\n",
    "    num_components: Number of principal components to return\n",
    "    \n",
    "    Returns:\n",
    "    X_transformed: Transformed data (samples × num_components)\n",
    "    eigenvectors: Principal components (features × num_components)\n",
    "    eigenvalues: Variance explained by each component\n",
    "    \"\"\"\n",
    "    # 1. Standardize the data\n",
    "    X_std = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    \n",
    "    # 2. Calculate covariance matrix\n",
    "    cov_matrix = np.cov(X_std.T)\n",
    "    \n",
    "    # 3. Calculate eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    \n",
    "    # 4. Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # 5. Select top k eigenvectors\n",
    "    top_k_eigenvectors = eigenvectors[:, :num_components]\n",
    "    \n",
    "    # 6. Transform the data\n",
    "    X_transformed = np.dot(X_std, top_k_eigenvectors)\n",
    "    \n",
    "    return X_transformed, top_k_eigenvectors, eigenvalues[:num_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to 2 dimensions\n",
    "X_pca, components, explained_variance = pca_from_scratch(X, 2)\n",
    "\n",
    "# Calculate the variance explained ratio\n",
    "total_variance = np.sum(explained_variance)\n",
    "explained_variance_ratio = explained_variance / total_variance\n",
    "\n",
    "print(f\"Explained variance ratio: {explained_variance_ratio}\")\n",
    "print(f\"Total variance explained: {np.sum(explained_variance_ratio) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot transformed data points with class colors\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "for i, color in enumerate(colors):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=0.8, lw=2,\n",
    "                label=iris.target_names[i])\n",
    "    \n",
    "# Add labels and title\n",
    "plt.xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')\n",
    "plt.title('PCA of Iris Dataset (Implemented from Scratch)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding PCA Step-by-Step\n",
    "\n",
    "1. **Standardization**: Center data by subtracting mean and scaling by standard deviation\n",
    "2. **Covariance Matrix**: Calculate how features vary with each other\n",
    "3. **Eigendecomposition**: Find eigenvalues and eigenvectors of covariance matrix\n",
    "4. **Sorting**: Arrange eigenvalues/vectors in descending order of eigenvalues\n",
    "5. **Selection**: Choose top k eigenvectors (principal components)\n",
    "6. **Transformation**: Project data onto new principal component space\n",
    "\n",
    "### Connection to Neural Networks\n",
    "\n",
    "- Linear layers in neural networks perform matrix multiplication: $y = Wx + b$\n",
    "- PCA finds directions of maximum variance, while neural networks learn representations for specific tasks\n",
    "- Both rely on linear algebra operations for transforming data into more useful representations\n",
    "- Auto-encoders with linear activations can learn PCA-like projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with sklearn's PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply sklearn's PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca_sklearn = pca.fit_transform(X)\n",
    "\n",
    "# Plot sklearn's PCA result\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, color in enumerate(colors):\n",
    "    plt.scatter(X_pca_sklearn[y == i, 0], X_pca_sklearn[y == i, 1], color=color, alpha=0.8, lw=2,\n",
    "                label=iris.target_names[i])\n",
    "    \n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title('PCA of Iris Dataset (Using sklearn)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"sklearn explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Our explained variance ratio: {explained_variance_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **Linear Algebra Basics**: Vectors, matrices, tensors and their operations\n",
    "2. **Matrix Operations**: Addition, multiplication, transpose, and other operations\n",
    "3. **Eigenvalues and Eigenvectors**: Their calculation and significance in data science\n",
    "4. **PCA Implementation**: From scratch using fundamental linear algebra concepts\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "- **Data Preprocessing**: PCA for dimensionality reduction\n",
    "- **Feature Engineering**: Creating meaningful representations\n",
    "- **Neural Networks**: Understanding the matrix operations behind layers\n",
    "- **Computer Vision**: Image processing using matrix operations\n",
    "- **Natural Language Processing**: Word embeddings as vectors in high-dimensional space\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "1. \"Deep Learning\" by Goodfellow, Bengio, and Courville - Chapter 2 on Linear Algebra\n",
    "2. \"Mathematics for Machine Learning\" by Deisenroth, Faisal, and Ong\n",
    "3. \"Linear Algebra and Its Applications\" by Gilbert Strang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}